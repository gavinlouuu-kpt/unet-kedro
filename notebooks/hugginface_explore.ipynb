{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamConfig {\n",
      "  \"_name_or_path\": \"facebook/sam-vit-base\",\n",
      "  \"architectures\": [\n",
      "    \"SamModel\"\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"mask_decoder_config\": {\n",
      "    \"model_type\": \"\"\n",
      "  },\n",
      "  \"model_type\": \"sam\",\n",
      "  \"prompt_encoder_config\": {\n",
      "    \"model_type\": \"\"\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"vision_config\": {\n",
      "    \"dropout\": 0.0,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"intermediate_size\": 6144,\n",
      "    \"model_type\": \"\",\n",
      "    \"projection_dim\": 512\n",
      "  }\n",
      "}\n",
      "\n",
      "Configuration file path: facebook/sam-vit-base.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'facebook/sam-vit-base.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m config_file_path \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_name_or_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguration file path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     14\u001b[0m     config_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(config_data, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\gavin\\anaconda3\\envs\\sam-kedro\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'facebook/sam-vit-base.json'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "import json\n",
    "\n",
    "model_name = \"facebook/sam-vit-base\"  # Example model name\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "print(config)\n",
    "\n",
    "config_file_path = config._name_or_path + \".json\"\n",
    "print(f\"Configuration file path: {config_file_path}\")\n",
    "\n",
    "with open(config_file_path, 'r') as file:\n",
    "    config_data = json.load(file)\n",
    "\n",
    "print(json.dumps(config_data, indent=4))\n",
    "\n",
    "assert config.to_dict() == config_data, \"Configuration mismatch!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"return_dict\": true,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_attentions\": false,\n",
      "    \"torchscript\": false,\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"use_bfloat16\": false,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"pruned_heads\": {},\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"is_decoder\": false,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"add_cross_attention\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"temperature\": 1.0,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_scores\": false,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"architectures\": [\n",
      "        \"SamModel\"\n",
      "    ],\n",
      "    \"finetuning_task\": null,\n",
      "    \"id2label\": {\n",
      "        \"0\": \"LABEL_0\",\n",
      "        \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"label2id\": {\n",
      "        \"LABEL_0\": 0,\n",
      "        \"LABEL_1\": 1\n",
      "    },\n",
      "    \"tokenizer_class\": null,\n",
      "    \"prefix\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"eos_token_id\": null,\n",
      "    \"sep_token_id\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"_name_or_path\": \"facebook/sam-vit-base\",\n",
      "    \"_attn_implementation_autoset\": false,\n",
      "    \"transformers_version\": \"4.47.0.dev0\",\n",
      "    \"model_type\": \"sam\",\n",
      "    \"vision_config\": {\n",
      "        \"return_dict\": true,\n",
      "        \"output_hidden_states\": false,\n",
      "        \"output_attentions\": false,\n",
      "        \"torchscript\": false,\n",
      "        \"torch_dtype\": null,\n",
      "        \"use_bfloat16\": false,\n",
      "        \"tf_legacy_loss\": false,\n",
      "        \"pruned_heads\": {},\n",
      "        \"tie_word_embeddings\": true,\n",
      "        \"chunk_size_feed_forward\": 0,\n",
      "        \"is_encoder_decoder\": false,\n",
      "        \"is_decoder\": false,\n",
      "        \"cross_attention_hidden_size\": null,\n",
      "        \"add_cross_attention\": false,\n",
      "        \"tie_encoder_decoder\": false,\n",
      "        \"max_length\": 20,\n",
      "        \"min_length\": 0,\n",
      "        \"do_sample\": false,\n",
      "        \"early_stopping\": false,\n",
      "        \"num_beams\": 1,\n",
      "        \"num_beam_groups\": 1,\n",
      "        \"diversity_penalty\": 0.0,\n",
      "        \"temperature\": 1.0,\n",
      "        \"top_k\": 50,\n",
      "        \"top_p\": 1.0,\n",
      "        \"typical_p\": 1.0,\n",
      "        \"repetition_penalty\": 1.0,\n",
      "        \"length_penalty\": 1.0,\n",
      "        \"no_repeat_ngram_size\": 0,\n",
      "        \"encoder_no_repeat_ngram_size\": 0,\n",
      "        \"bad_words_ids\": null,\n",
      "        \"num_return_sequences\": 1,\n",
      "        \"output_scores\": false,\n",
      "        \"return_dict_in_generate\": false,\n",
      "        \"forced_bos_token_id\": null,\n",
      "        \"forced_eos_token_id\": null,\n",
      "        \"remove_invalid_values\": false,\n",
      "        \"exponential_decay_length_penalty\": null,\n",
      "        \"suppress_tokens\": null,\n",
      "        \"begin_suppress_tokens\": null,\n",
      "        \"architectures\": null,\n",
      "        \"finetuning_task\": null,\n",
      "        \"id2label\": {\n",
      "            \"0\": \"LABEL_0\",\n",
      "            \"1\": \"LABEL_1\"\n",
      "        },\n",
      "        \"label2id\": {\n",
      "            \"LABEL_0\": 0,\n",
      "            \"LABEL_1\": 1\n",
      "        },\n",
      "        \"tokenizer_class\": null,\n",
      "        \"prefix\": null,\n",
      "        \"bos_token_id\": null,\n",
      "        \"pad_token_id\": null,\n",
      "        \"eos_token_id\": null,\n",
      "        \"sep_token_id\": null,\n",
      "        \"decoder_start_token_id\": null,\n",
      "        \"task_specific_params\": null,\n",
      "        \"problem_type\": null,\n",
      "        \"_name_or_path\": \"\",\n",
      "        \"_attn_implementation_autoset\": false,\n",
      "        \"dropout\": 0.0,\n",
      "        \"initializer_factor\": 1.0,\n",
      "        \"intermediate_size\": 6144,\n",
      "        \"model_type\": \"\",\n",
      "        \"projection_dim\": 512,\n",
      "        \"hidden_size\": 768,\n",
      "        \"output_channels\": 256,\n",
      "        \"num_hidden_layers\": 12,\n",
      "        \"num_attention_heads\": 12,\n",
      "        \"num_channels\": 3,\n",
      "        \"image_size\": 1024,\n",
      "        \"patch_size\": 16,\n",
      "        \"hidden_act\": \"gelu\",\n",
      "        \"layer_norm_eps\": 1e-06,\n",
      "        \"attention_dropout\": 0.0,\n",
      "        \"initializer_range\": 1e-10,\n",
      "        \"qkv_bias\": true,\n",
      "        \"mlp_ratio\": 4.0,\n",
      "        \"use_abs_pos\": true,\n",
      "        \"use_rel_pos\": true,\n",
      "        \"window_size\": 14,\n",
      "        \"global_attn_indexes\": [\n",
      "            2,\n",
      "            5,\n",
      "            8,\n",
      "            11\n",
      "        ],\n",
      "        \"num_pos_feats\": 128,\n",
      "        \"mlp_dim\": 3072\n",
      "    },\n",
      "    \"prompt_encoder_config\": {\n",
      "        \"return_dict\": true,\n",
      "        \"output_hidden_states\": false,\n",
      "        \"output_attentions\": false,\n",
      "        \"torchscript\": false,\n",
      "        \"torch_dtype\": null,\n",
      "        \"use_bfloat16\": false,\n",
      "        \"tf_legacy_loss\": false,\n",
      "        \"pruned_heads\": {},\n",
      "        \"tie_word_embeddings\": true,\n",
      "        \"chunk_size_feed_forward\": 0,\n",
      "        \"is_encoder_decoder\": false,\n",
      "        \"is_decoder\": false,\n",
      "        \"cross_attention_hidden_size\": null,\n",
      "        \"add_cross_attention\": false,\n",
      "        \"tie_encoder_decoder\": false,\n",
      "        \"max_length\": 20,\n",
      "        \"min_length\": 0,\n",
      "        \"do_sample\": false,\n",
      "        \"early_stopping\": false,\n",
      "        \"num_beams\": 1,\n",
      "        \"num_beam_groups\": 1,\n",
      "        \"diversity_penalty\": 0.0,\n",
      "        \"temperature\": 1.0,\n",
      "        \"top_k\": 50,\n",
      "        \"top_p\": 1.0,\n",
      "        \"typical_p\": 1.0,\n",
      "        \"repetition_penalty\": 1.0,\n",
      "        \"length_penalty\": 1.0,\n",
      "        \"no_repeat_ngram_size\": 0,\n",
      "        \"encoder_no_repeat_ngram_size\": 0,\n",
      "        \"bad_words_ids\": null,\n",
      "        \"num_return_sequences\": 1,\n",
      "        \"output_scores\": false,\n",
      "        \"return_dict_in_generate\": false,\n",
      "        \"forced_bos_token_id\": null,\n",
      "        \"forced_eos_token_id\": null,\n",
      "        \"remove_invalid_values\": false,\n",
      "        \"exponential_decay_length_penalty\": null,\n",
      "        \"suppress_tokens\": null,\n",
      "        \"begin_suppress_tokens\": null,\n",
      "        \"architectures\": null,\n",
      "        \"finetuning_task\": null,\n",
      "        \"id2label\": {\n",
      "            \"0\": \"LABEL_0\",\n",
      "            \"1\": \"LABEL_1\"\n",
      "        },\n",
      "        \"label2id\": {\n",
      "            \"LABEL_0\": 0,\n",
      "            \"LABEL_1\": 1\n",
      "        },\n",
      "        \"tokenizer_class\": null,\n",
      "        \"prefix\": null,\n",
      "        \"bos_token_id\": null,\n",
      "        \"pad_token_id\": null,\n",
      "        \"eos_token_id\": null,\n",
      "        \"sep_token_id\": null,\n",
      "        \"decoder_start_token_id\": null,\n",
      "        \"task_specific_params\": null,\n",
      "        \"problem_type\": null,\n",
      "        \"_name_or_path\": \"\",\n",
      "        \"_attn_implementation_autoset\": false,\n",
      "        \"image_embedding_size\": 64,\n",
      "        \"model_type\": \"\",\n",
      "        \"hidden_size\": 256,\n",
      "        \"image_size\": 1024,\n",
      "        \"patch_size\": 16,\n",
      "        \"mask_input_channels\": 16,\n",
      "        \"num_point_embeddings\": 4,\n",
      "        \"hidden_act\": \"gelu\",\n",
      "        \"layer_norm_eps\": 1e-06\n",
      "    },\n",
      "    \"mask_decoder_config\": {\n",
      "        \"return_dict\": true,\n",
      "        \"output_hidden_states\": false,\n",
      "        \"output_attentions\": false,\n",
      "        \"torchscript\": false,\n",
      "        \"torch_dtype\": null,\n",
      "        \"use_bfloat16\": false,\n",
      "        \"tf_legacy_loss\": false,\n",
      "        \"pruned_heads\": {},\n",
      "        \"tie_word_embeddings\": true,\n",
      "        \"chunk_size_feed_forward\": 0,\n",
      "        \"is_encoder_decoder\": false,\n",
      "        \"is_decoder\": false,\n",
      "        \"cross_attention_hidden_size\": null,\n",
      "        \"add_cross_attention\": false,\n",
      "        \"tie_encoder_decoder\": false,\n",
      "        \"max_length\": 20,\n",
      "        \"min_length\": 0,\n",
      "        \"do_sample\": false,\n",
      "        \"early_stopping\": false,\n",
      "        \"num_beams\": 1,\n",
      "        \"num_beam_groups\": 1,\n",
      "        \"diversity_penalty\": 0.0,\n",
      "        \"temperature\": 1.0,\n",
      "        \"top_k\": 50,\n",
      "        \"top_p\": 1.0,\n",
      "        \"typical_p\": 1.0,\n",
      "        \"repetition_penalty\": 1.0,\n",
      "        \"length_penalty\": 1.0,\n",
      "        \"no_repeat_ngram_size\": 0,\n",
      "        \"encoder_no_repeat_ngram_size\": 0,\n",
      "        \"bad_words_ids\": null,\n",
      "        \"num_return_sequences\": 1,\n",
      "        \"output_scores\": false,\n",
      "        \"return_dict_in_generate\": false,\n",
      "        \"forced_bos_token_id\": null,\n",
      "        \"forced_eos_token_id\": null,\n",
      "        \"remove_invalid_values\": false,\n",
      "        \"exponential_decay_length_penalty\": null,\n",
      "        \"suppress_tokens\": null,\n",
      "        \"begin_suppress_tokens\": null,\n",
      "        \"architectures\": null,\n",
      "        \"finetuning_task\": null,\n",
      "        \"id2label\": {\n",
      "            \"0\": \"LABEL_0\",\n",
      "            \"1\": \"LABEL_1\"\n",
      "        },\n",
      "        \"label2id\": {\n",
      "            \"LABEL_0\": 0,\n",
      "            \"LABEL_1\": 1\n",
      "        },\n",
      "        \"tokenizer_class\": null,\n",
      "        \"prefix\": null,\n",
      "        \"bos_token_id\": null,\n",
      "        \"pad_token_id\": null,\n",
      "        \"eos_token_id\": null,\n",
      "        \"sep_token_id\": null,\n",
      "        \"decoder_start_token_id\": null,\n",
      "        \"task_specific_params\": null,\n",
      "        \"problem_type\": null,\n",
      "        \"_name_or_path\": \"\",\n",
      "        \"_attn_implementation_autoset\": false,\n",
      "        \"model_type\": \"\",\n",
      "        \"hidden_size\": 256,\n",
      "        \"hidden_act\": \"relu\",\n",
      "        \"mlp_dim\": 2048,\n",
      "        \"num_hidden_layers\": 2,\n",
      "        \"num_attention_heads\": 8,\n",
      "        \"attention_downsample_rate\": 2,\n",
      "        \"num_multimask_outputs\": 3,\n",
      "        \"iou_head_depth\": 3,\n",
      "        \"iou_head_hidden_dim\": 256,\n",
      "        \"layer_norm_eps\": 1e-06\n",
      "    },\n",
      "    \"initializer_range\": 0.02\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "import json\n",
    "\n",
    "model_name = \"facebook/sam-vit-base\"  # Example model name\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config_dict = config.to_dict()\n",
    "print(json.dumps(config_dict, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "model_name = \"facebook/sam-vit-base\"  # Example model name\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamVisionConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"global_attn_indexes\": [\n",
      "    2,\n",
      "    5,\n",
      "    8,\n",
      "    11\n",
      "  ],\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 1024,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 1e-10,\n",
      "  \"intermediate_size\": 6144,\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"mlp_dim\": 3072,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_pos_feats\": 128,\n",
      "  \"output_channels\": 256,\n",
      "  \"patch_size\": 16,\n",
      "  \"projection_dim\": 512,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_abs_pos\": true,\n",
      "  \"use_rel_pos\": true,\n",
      "  \"window_size\": 14\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Accessing the vision configuration\n",
    "vision_config = config.vision_config\n",
    "print(vision_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "model_name = \"facebook/sam-vit-base\"  # Example model name\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamVisionConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"global_attn_indexes\": [\n",
      "    2,\n",
      "    5,\n",
      "    8,\n",
      "    11\n",
      "  ],\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 1024,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 1e-10,\n",
      "  \"intermediate_size\": 6144,\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"mlp_dim\": 3072,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_pos_feats\": 128,\n",
      "  \"output_channels\": 256,\n",
      "  \"patch_size\": 16,\n",
      "  \"projection_dim\": 512,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_abs_pos\": true,\n",
      "  \"use_rel_pos\": true,\n",
      "  \"window_size\": 14\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Accessing the vision configuration\n",
    "vision_config = config.vision_config\n",
    "print(vision_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamVisionConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"global_attn_indexes\": [\n",
      "    2,\n",
      "    5,\n",
      "    8,\n",
      "    11\n",
      "  ],\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 1024,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 1e-10,\n",
      "  \"intermediate_size\": 6144,\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"mlp_dim\": 3072,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_pos_feats\": 128,\n",
      "  \"output_channels\": 256,\n",
      "  \"patch_size\": 16,\n",
      "  \"projection_dim\": 512,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_abs_pos\": true,\n",
      "  \"use_rel_pos\": true,\n",
      "  \"window_size\": 14\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Modifying the dropout rate in the vision configuration\n",
    "config.vision_config.dropout = 0.1\n",
    "print(config.vision_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-kedro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
